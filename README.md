# Survey on Natural Language Processing

> "It takes all the running you can do, to keep in the same place."

<br>

## 文献リスト
### Language Understanding

- [Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.](https://github.com/marucha80t/survey-nlp/blob/master/docs/GPT.md)
- [Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So and Jaewoo Kang. 2019. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. arXiv preprint. arXiv:1901.08746](https://github.com/marucha80t/survey-nlp/blob/master/docs/BioBERT.md)
- [Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE:Enhanced Language Representation with Informative Entities. In Proceedings of ACL 2019.](https://github.com/marucha80t/survey-nlp/blob/master/docs/ERNIE.md)
- [Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint. arXiv:1907.11692.](https://github.com/marucha80t/survey-nlp/blob/master/docs/RoBERTa.md)
