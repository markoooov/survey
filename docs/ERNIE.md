# ERNIE: Enhanced Representation through Knowledge Integration

BERTにおけるマスク単語予測を、phrase-level, entity-levelに拡張した(単語よりもまとまった単位のマスクを行う)モデルERNIEを提案。NLIやNERをはじめとする5種類のタスクで、BERTを超える性能を達成。またクローズテストにおいても、BERTに比べて、より高度な推論が可能であることを示した。
<br>

![ernie_masking](https://user-images.githubusercontent.com/53220859/62587934-e84bda80-b8fe-11e9-88fb-be0120227d6f.png)
※ 図は元論文を引用。
<br>

## 文献情報

- 著者: Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu
- リンク: [https://arxiv.org/abs/1904.09223](https://arxiv.org/abs/1904.09223)

