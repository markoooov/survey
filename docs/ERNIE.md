# ERNIE: Enhanced Representation through Knowledge Integration

BERTにおけるマスク単語予測を、phrase-level, entity-levelに拡張した(単語よりもまとまった単位のマスクを行う)モデルERNIEを提案。NLIやNERをはじめとする5種類のタスクで、BERTを超える性能を達成。またクローズテストにおいても、BERTに比べて、より高度な推論が可能であることを示した。



※ 図は元論文を引用。



## 文献情報

- 著者: Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu
- リンク: [https://arxiv.org/abs/1904.09223](https://arxiv.org/abs/1904.09223)

