# ERNIE: Enhanced Representation through Knowledge Integration
BERTにおけるマスク単語予測を、phrase-level, entity-levelに拡張した(単語よりもまとまった単位のマスクを行う)モデルERNIEを提案。NLIやNERをはじめとする5種類のタスクで、BERTを超える性能を達成。またクローズテストにおいても、BERTに比べて、より高度な推論が可能であることを示した。

<p align="center">
<img src=https://user-images.githubusercontent.com/53220859/62587934-e84bda80-b8fe-11e9-88fb-be0120227d6f.png width=900pt>
</p>


## 文献情報
- 著者: Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu
- リンク: [https://arxiv.org/abs/1904.09223](https://arxiv.org/abs/1904.09223)
- 学会: arXiv2019
